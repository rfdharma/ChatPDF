{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "\n",
    "# Load & process\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# Vector store & embeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Conversations\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Post-processing\n",
    "import fitz\n",
    "\n",
    "# Token counter\n",
    "import tiktoken\n",
    "encoder = tiktoken.encoding_for_model(\"text-embedding-ada-002\")\n",
    "from langchain.callbacks.manager import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF\n",
    "loader = PyPDFLoader('pdfs/2309.13963.pdf')\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into chunks\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 200\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create embeddings from chunks\n",
    "embeddings = OpenAIEmbeddings()\n",
    "docsearch = Chroma.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "chat_model = ChatOpenAI()\n",
    "llm = OpenAI()\n",
    "\n",
    "# Template prompt\n",
    "template = \"\"\"Based ONLY on the context below, answer the following question!\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a nice chatbot having a conversation with a human.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(template),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contexts_formatter(contexts):\n",
    "    result = \"\"\n",
    "    for i in range(len(contexts)):\n",
    "        result += f\"{i+1}. {contexts[i].page_content}\\n\\n\\n\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='3. MODULE CONNECTOR\\nAs shown in Fig. 1, the proposed ASR model consists of three mod-\\nules: a frozen speech encoder, a trainable module connector and\\na frozen LLM. This section introduces three connectors, including\\nfully connected layers, multi-head cross-attention and Q-Former.\\n❄ Speech EncoderLinearMHCA\\n❄ Large Language Model(a)(b)(c)Transcription\\nQ-FormerQ-Former queryVicuna embeddingConv kernel\\nLinear\\nFig. 1 . Illustration of integrating a speech encoder and an LLM into\\nan ASR system with a module connector of: (a) fully connected\\nlayers, (b) multi-head cross-attention, and (c) Q-Former.\\nFor clarity, some basic notations are defined as follows: X∈\\nRnx×dxdenotes the speech features obtained from the speech en-\\ncoder, and the module connector compresses XintoTspeech∈\\nRnt×dtwhich are input to the LLM to produce ASR transcriptions.\\nH∈Rnh×dhdenotes the hidden states in connectors while nandd\\nare the numbers of vectors and hidden dimensions respectively.\\n3.1. Fully connected layers\\nTo compress the length of speech features, madjacent frames xi,\\nxi+1, ...,xi+m−1are stacked into hi∈Rm×dx. Then two Linear (·)\\nlayers with ReLU (·)in between are introduced as follows:\\nTspeech=Linear (ReLU (Linear (H))), (1)\\nwhere Hconsists of hiof a batch of samples. Actually, the vector\\nstacking operation together with the first linear layer works the same\\nas a 1-dimensional (-d) convolutional layer, Conv1d (·).\\n3.2. Multi-head cross-attention\\nTo bridge the gap between the multi-modal encoder output features\\nXand LLM input textual features Tspeech, a multi-head attention\\nlayer [33] denoted as MultiHead (Query ,Key,Value )is used in the\\nmulti-head cross-attention approach to align the two feature spaces\\n[19]. First, a Conv1d (·)layer reduces the length of the speech input\\nby a rate of s. Then the hidden states Hare converted to Tspeech\\nbased on the textual embeddings Eusing MultiHead (·). That is,\\nH=Linear (Conv1d (X)) (2)\\nTspeech=MultiHead (H,E,E). (3)3.3. Q-Former\\nQ-Former [20] is a Transformer-based module converting variable-\\nlength input sequences into fixed-length output query representa-\\ntions. It was initially proposed for visual-text modality alignment,\\nand here is applied to audio-text alignment. In each Q-Former block,\\ntrainable query embeddings Q∈Rnq×dqinteract with the input fea-\\nturesXthrough multi-head self-attention and cross-attention layers,\\nSpecifically, Q-Former, denoted as QF (Q,X), in this work consists\\nof two Transformer decoder blocks [33] with the causal attention\\nmasks removed. Here Qis used as the decoder inputs and Xas the\\nencoder outputs in the standard Transformer.\\n4. SEGMENT-LEVEL Q-FORMER\\nTransformer-based speech encoders can have limitations on the in-\\nput sequence duration [18]. To enable LLMs to process with longer\\nspeech inputs, the whole sequence can be split into several shorter\\nsegments to transform by the speech encoder separately. Such seg-\\nments can be concatenated to reform a single sequence at either\\nthe input or output end of Q-Former. In this paper, the structure\\nof segment-level Q-Former (seg-QF) shown in Fig. 2 is proposed,\\nwhich uses a Q-Former to transform each encoder output segment\\nsimultaneously and concatenates their fixed-length output token se-\\nquences before feeding into the LLM. Compared to performing the\\nconcatenation at the Q-Former input end and producing a fixed num-\\nber of nqoutput tokens, seg-QF allows varying the number of out-\\nput tokens N×nqaccording to the number of segments N, which\\nis more suitable for speech inputs with variable lengths in a wide\\nrange. Note the trainable query embeddings Q', metadata={'page': 1, 'source': 'pdfs/2309.13963.pdf'}),\n",
       " Document(page_content=' sequence can be split into several shorter\\nsegments to transform by the speech encoder separately. Such seg-\\nments can be concatenated to reform a single sequence at either\\nthe input or output end of Q-Former. In this paper, the structure\\nof segment-level Q-Former (seg-QF) shown in Fig. 2 is proposed,\\nwhich uses a Q-Former to transform each encoder output segment\\nsimultaneously and concatenates their fixed-length output token se-\\nquences before feeding into the LLM. Compared to performing the\\nconcatenation at the Q-Former input end and producing a fixed num-\\nber of nqoutput tokens, seg-QF allows varying the number of out-\\nput tokens N×nqaccording to the number of segments N, which\\nis more suitable for speech inputs with variable lengths in a wide\\nrange. Note the trainable query embeddings Qand Q-Former lay-\\ners are shared among all the segments, and seg-QF can be initialised\\nwith a pre-trained standard Q-Former.\\n❄ Speech Encoder12Seg-QF\\n❄ Large Language ModelTranscription\\n………pos:pos:\\nFig. 2 . The model structure of segment-level Q-Former (seg-QF).\\nThe integers in rectangles are segment-level positional encodings.\\nDespite that relative positions of the frames are provided by the\\nspeech encoder within each segment Si∈Rnx×dx, Seg-QF is not\\naware of their absolute positions in the whole input sequence. To\\ninform Seg-QF with such information, segment-level position em-\\nbeddings pi∈Rdxare added to X, as shown in Fig. 2. Specifically,\\nTspeech= [QF(Q,Si⊕pi)]N\\ni=1, (4)\\nwhere Si⊕qimeans adding qito each row of Si.', metadata={'page': 1, 'source': 'pdfs/2309.13963.pdf'}),\n",
       " Document(page_content='F† FC QF Seg-QF Seg-QF*\\ntest-clean 3.00 2.28 2.89 2.32 2.35 2.19\\nt⩽60 58.18 61.99 3.43 3.83 3.11 2.68\\nt⩽90 70.64 75.45 4.98 5.40 3.81 3.24\\nt⩽120 75.48 76.81 19.14 26.91 17.95 13.60\\ntest-other 6.70 5.20 6.68 5.40 5.37 5.20\\nt⩽60 62.19 63.49 7.58 7.81 6.43 5.13\\nt⩽90 74.27 76.03 9.03 11.78 8.07 6.17\\nt⩽120 78.75 76.14 24.55 32.24 22.25 19.63\\nTable 6 . %WERs on the concatenated test sets. Results based on\\nthe Librispeech test clean and test other test sets are shown in Row\\n1-4 and Row 5-8 respectively, and Row 1 and Row 5 are the results\\nwith the original segmentation. Each t⩽Ttesttest set is obtained by\\nconcatenating either test clean or test other utterances based on their\\norders that appeared in the original audiobooks to up to Ttestseconds.\\nModels with † were not fine-tuned on concatenated training data.\\nPre-trained checkpoints for Seg-QF* were QF in Table 5, and were\\nFC in Row 1 and QF in Row 5 of Table 2 for other models.\\noriginal  60\\n  90\\n/s\\nduration45678 %WER\\nall data\\nsuccessfully\\ndecoded data\\noriginal  60\\n  90\\n/s\\nduration56 %WER\\nall data\\nsuccessfully\\ndecoded data\\nFig. 3 . %WERs of Seg-QF ( left) and Seg-QF* ( right ) in Tab. 6\\ncalculated on all data and successfully decoded data in the test-other\\nset. The x-axis denotes the duration of the concatenated audio.\\n120-second-long speech inputs. From Table 6, WERs degrade with\\nlonger speech inputs. More extreme cases, such as repeated out-\\nputs and large chunks of deletions, were observed in case studies\\nwhen feeding long speech inputs. The WERs for all utterances and\\nthose successfully decoded in Librispeech test-other are plotted in\\nFig. 3 for Seg-QF and Seg-QF*. It shows that the contexts in long\\nspeech inputs help ASR to reduce WERs if being decoded success-\\nfully. However, the overall WERs on the full test-other set increased\\ndue to more frequent extreme cases.\\n7. CONCLUSION\\nThis paper studies to enable LLMs to recognise speech inputs by\\ninterfacing with a speech encoder. Three commonly used connectors\\nincluding fully-connected layers, multi-head cross-attention and Q-\\nFormer were compared. The LLMs with Q-Formers demonstrated\\nsuperior performance over LLMs with other connectors and the\\nWhisper baseline ASR system on all of the in-domain and out-of-\\ndomain test sets. Moreover, a novel segment-level Q-Former was\\nproposed to improve the performance with long-form speech inputs\\nwhose duration exceeds the limitations of the pre-trained speech\\nencoder. Analyses show that although the rich context information\\nin long-form speech inputs can improve ASR accuracy, overly long\\ninputs can also aggravate the hallucination problem of LLMs.', metadata={'page': 3, 'source': 'pdfs/2309.13963.pdf'})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query\n",
    "question = \"jelasin apa itu q-former\"\n",
    "\n",
    "# Vanilla\n",
    "contexts = docsearch.similarity_search(question, k=3)\n",
    "contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokens Used: 145\n",
       "\tPrompt Tokens: 99\n",
       "\tCompletion Tokens: 46\n",
       "Successful Requests: 1\n",
       "Total Cost (USD): $0.0029"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiquery Retriever\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=docsearch.as_retriever(), llm=llm\n",
    ")\n",
    "with get_openai_callback() as cb:\n",
    "    contexts = retriever_from_llm.get_relevant_documents(query=question, k=3)\n",
    "cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maula\\Documents\\Project\\uas_nlp\\chatbot\\lib\\site-packages\\langchain\\chains\\llm.py:321: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "c:\\Users\\maula\\Documents\\Project\\uas_nlp\\chatbot\\lib\\site-packages\\langchain\\chains\\llm.py:321: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "c:\\Users\\maula\\Documents\\Project\\uas_nlp\\chatbot\\lib\\site-packages\\langchain\\chains\\llm.py:321: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "c:\\Users\\maula\\Documents\\Project\\uas_nlp\\chatbot\\lib\\site-packages\\langchain\\chains\\llm.py:321: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tokens Used: 2827\n",
       "\tPrompt Tokens: 2574\n",
       "\tCompletion Tokens: 253\n",
       "Successful Requests: 4\n",
       "Total Cost (USD): $0.05654000000000001"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contextual Compression\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=docsearch.as_retriever())\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    contexts = compression_retriever.get_relevant_documents(question, k=3)\n",
    "cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokens Used: 1243\n",
       "\tPrompt Tokens: 1206\n",
       "\tCompletion Tokens: 37\n",
       "Successful Requests: 1\n",
       "Total Cost (USD): $0.02486"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "\n",
    "document_content_description = \"Sebuah researh paper\"\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"page number\",\n",
    "        type=\"integer\"\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"file path\",\n",
    "        type=\"string\"\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    docsearch,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    enable_limit=True\n",
    ")\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    retriever.get_relevant_documents(\"3 dokumen tentang Q-Former\", k=3)\n",
    "cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='3. MODULE CONNECTOR\\nAs shown in Fig. 1, the proposed ASR model consists of three mod-\\nules: a frozen speech encoder, a trainable module connector and\\na frozen LLM. This section introduces three connectors, including\\nfully connected layers, multi-head cross-attention and Q-Former.\\n❄ Speech EncoderLinearMHCA\\n❄ Large Language Model(a)(b)(c)Transcription\\nQ-FormerQ-Former queryVicuna embeddingConv kernel\\nLinear\\nFig. 1 . Illustration of integrating a speech encoder and an LLM into\\nan ASR system with a module connector of: (a) fully connected\\nlayers, (b) multi-head cross-attention, and (c) Q-Former.\\nFor clarity, some basic notations are defined as follows: X∈\\nRnx×dxdenotes the speech features obtained from the speech en-\\ncoder, and the module connector compresses XintoTspeech∈\\nRnt×dtwhich are input to the LLM to produce ASR transcriptions.\\nH∈Rnh×dhdenotes the hidden states in connectors while nandd\\nare the numbers of vectors and hidden dimensions respectively.\\n3.1. Fully connected layers\\nTo compress the length of speech features, madjacent frames xi,\\nxi+1, ...,xi+m−1are stacked into hi∈Rm×dx. Then two Linear (·)\\nlayers with ReLU (·)in between are introduced as follows:\\nTspeech=Linear (ReLU (Linear (H))), (1)\\nwhere Hconsists of hiof a batch of samples. Actually, the vector\\nstacking operation together with the first linear layer works the same\\nas a 1-dimensional (-d) convolutional layer, Conv1d (·).\\n3.2. Multi-head cross-attention\\nTo bridge the gap between the multi-modal encoder output features\\nXand LLM input textual features Tspeech, a multi-head attention\\nlayer [33] denoted as MultiHead (Query ,Key,Value )is used in the\\nmulti-head cross-attention approach to align the two feature spaces\\n[19]. First, a Conv1d (·)layer reduces the length of the speech input\\nby a rate of s. Then the hidden states Hare converted to Tspeech\\nbased on the textual embeddings Eusing MultiHead (·). That is,\\nH=Linear (Conv1d (X)) (2)\\nTspeech=MultiHead (H,E,E). (3)3.3. Q-Former\\nQ-Former [20] is a Transformer-based module converting variable-\\nlength input sequences into fixed-length output query representa-\\ntions. It was initially proposed for visual-text modality alignment,\\nand here is applied to audio-text alignment. In each Q-Former block,\\ntrainable query embeddings Q∈Rnq×dqinteract with the input fea-\\nturesXthrough multi-head self-attention and cross-attention layers,\\nSpecifically, Q-Former, denoted as QF (Q,X), in this work consists\\nof two Transformer decoder blocks [33] with the causal attention\\nmasks removed. Here Qis used as the decoder inputs and Xas the\\nencoder outputs in the standard Transformer.\\n4. SEGMENT-LEVEL Q-FORMER\\nTransformer-based speech encoders can have limitations on the in-\\nput sequence duration [18]. To enable LLMs to process with longer\\nspeech inputs, the whole sequence can be split into several shorter\\nsegments to transform by the speech encoder separately. Such seg-\\nments can be concatenated to reform a single sequence at either\\nthe input or output end of Q-Former. In this paper, the structure\\nof segment-level Q-Former (seg-QF) shown in Fig. 2 is proposed,\\nwhich uses a Q-Former to transform each encoder output segment\\nsimultaneously and concatenates their fixed-length output token se-\\nquences before feeding into the LLM. Compared to performing the\\nconcatenation at the Q-Former input end and producing a fixed num-\\nber of nqoutput tokens, seg-QF allows varying the number of out-\\nput tokens N×nqaccording to the number of segments N, which\\nis more suitable for speech inputs with variable lengths in a wide\\nrange. Note the trainable query embeddings Q', metadata={'page': 1, 'source': 'pdfs/2309.13963.pdf'}),\n",
       " Document(page_content=' sequence can be split into several shorter\\nsegments to transform by the speech encoder separately. Such seg-\\nments can be concatenated to reform a single sequence at either\\nthe input or output end of Q-Former. In this paper, the structure\\nof segment-level Q-Former (seg-QF) shown in Fig. 2 is proposed,\\nwhich uses a Q-Former to transform each encoder output segment\\nsimultaneously and concatenates their fixed-length output token se-\\nquences before feeding into the LLM. Compared to performing the\\nconcatenation at the Q-Former input end and producing a fixed num-\\nber of nqoutput tokens, seg-QF allows varying the number of out-\\nput tokens N×nqaccording to the number of segments N, which\\nis more suitable for speech inputs with variable lengths in a wide\\nrange. Note the trainable query embeddings Qand Q-Former lay-\\ners are shared among all the segments, and seg-QF can be initialised\\nwith a pre-trained standard Q-Former.\\n❄ Speech Encoder12Seg-QF\\n❄ Large Language ModelTranscription\\n………pos:pos:\\nFig. 2 . The model structure of segment-level Q-Former (seg-QF).\\nThe integers in rectangles are segment-level positional encodings.\\nDespite that relative positions of the frames are provided by the\\nspeech encoder within each segment Si∈Rnx×dx, Seg-QF is not\\naware of their absolute positions in the whole input sequence. To\\ninform Seg-QF with such information, segment-level position em-\\nbeddings pi∈Rdxare added to X, as shown in Fig. 2. Specifically,\\nTspeech= [QF(Q,Si⊕pi)]N\\ni=1, (4)\\nwhere Si⊕qimeans adding qito each row of Si.', metadata={'page': 1, 'source': 'pdfs/2309.13963.pdf'}),\n",
       " Document(page_content='F† FC QF Seg-QF Seg-QF*\\ntest-clean 3.00 2.28 2.89 2.32 2.35 2.19\\nt⩽60 58.18 61.99 3.43 3.83 3.11 2.68\\nt⩽90 70.64 75.45 4.98 5.40 3.81 3.24\\nt⩽120 75.48 76.81 19.14 26.91 17.95 13.60\\ntest-other 6.70 5.20 6.68 5.40 5.37 5.20\\nt⩽60 62.19 63.49 7.58 7.81 6.43 5.13\\nt⩽90 74.27 76.03 9.03 11.78 8.07 6.17\\nt⩽120 78.75 76.14 24.55 32.24 22.25 19.63\\nTable 6 . %WERs on the concatenated test sets. Results based on\\nthe Librispeech test clean and test other test sets are shown in Row\\n1-4 and Row 5-8 respectively, and Row 1 and Row 5 are the results\\nwith the original segmentation. Each t⩽Ttesttest set is obtained by\\nconcatenating either test clean or test other utterances based on their\\norders that appeared in the original audiobooks to up to Ttestseconds.\\nModels with † were not fine-tuned on concatenated training data.\\nPre-trained checkpoints for Seg-QF* were QF in Table 5, and were\\nFC in Row 1 and QF in Row 5 of Table 2 for other models.\\noriginal  60\\n  90\\n/s\\nduration45678 %WER\\nall data\\nsuccessfully\\ndecoded data\\noriginal  60\\n  90\\n/s\\nduration56 %WER\\nall data\\nsuccessfully\\ndecoded data\\nFig. 3 . %WERs of Seg-QF ( left) and Seg-QF* ( right ) in Tab. 6\\ncalculated on all data and successfully decoded data in the test-other\\nset. The x-axis denotes the duration of the concatenated audio.\\n120-second-long speech inputs. From Table 6, WERs degrade with\\nlonger speech inputs. More extreme cases, such as repeated out-\\nputs and large chunks of deletions, were observed in case studies\\nwhen feeding long speech inputs. The WERs for all utterances and\\nthose successfully decoded in Librispeech test-other are plotted in\\nFig. 3 for Seg-QF and Seg-QF*. It shows that the contexts in long\\nspeech inputs help ASR to reduce WERs if being decoded success-\\nfully. However, the overall WERs on the full test-other set increased\\ndue to more frequent extreme cases.\\n7. CONCLUSION\\nThis paper studies to enable LLMs to recognise speech inputs by\\ninterfacing with a speech encoder. Three commonly used connectors\\nincluding fully-connected layers, multi-head cross-attention and Q-\\nFormer were compared. The LLMs with Q-Formers demonstrated\\nsuperior performance over LLMs with other connectors and the\\nWhisper baseline ASR system on all of the in-domain and out-of-\\ndomain test sets. Moreover, a novel segment-level Q-Former was\\nproposed to improve the performance with long-form speech inputs\\nwhose duration exceeds the limitations of the pre-trained speech\\nencoder. Analyses show that although the rich context information\\nin long-form speech inputs can improve ASR accuracy, overly long\\ninputs can also aggravate the hallucination problem of LLMs.', metadata={'page': 3, 'source': 'pdfs/2309.13963.pdf'}),\n",
       " Document(page_content='.', metadata={'page': 3, 'source': 'pdfs/2309.13963.pdf'})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = docsearch.as_retriever(type=\"mmr\")\n",
    "retriever.get_relevant_documents(question, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "store = InMemoryStore()\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=docsearch,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")\n",
    "retriever.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Specifically, Q-Former, denoted as QF (Q,X), in this work consists\\nof two Transformer decoder blocks [33] with the causal attention\\nmasks removed. Here Qis used as the decoder inputs and Xas the\\nencoder outputs in the standard Transformer.\\n4. SEGMENT-LEVEL Q-FORMER\\nTransformer-based speech encoders can have limitations on the in-\\nput sequence duration [18]. To enable LLMs to process with longer', metadata={'doc_id': 'ee1bd7b1-3660-47fa-ae10-ad681c76a289', 'page': 1, 'source': 'pdfs/2309.13963.pdf'}),\n",
       " Document(page_content='and here is applied to audio-text alignment. In each Q-Former block,\\ntrainable query embeddings Q∈Rnq×dqinteract with the input fea-\\nturesXthrough multi-head self-attention and cross-attention layers,\\nSpecifically, Q-Former, denoted as QF (Q,X), in this work consists\\nof two Transformer decoder blocks [33] with the causal attention\\nmasks removed. Here Qis used as the decoder inputs and Xas the', metadata={'doc_id': 'ee1bd7b1-3660-47fa-ae10-ad681c76a289', 'page': 1, 'source': 'pdfs/2309.13963.pdf'}),\n",
       " Document(page_content='V oice, and GigaSpeech datasets, where the LLMs with Q-Formers\\ndemonstrated consistent and considerable word error rate (WER)\\nreductions over LLMs with other connector structures. Q-Former-\\nbased LLMs can generalise well to out-of-domain datasets, where\\n12% relative WER reductions over the Whisper baseline ASR model\\nwere achieved on the Eval2000 test set without using any in-domain', metadata={'doc_id': 'aefd780d-b550-4eb9-a4f4-e7405c2a9ae3', 'page': 0, 'source': 'pdfs/2309.13963.pdf'}),\n",
       " Document(page_content='Tspeech=MultiHead (H,E,E). (3)3.3. Q-Former\\nQ-Former [20] is a Transformer-based module converting variable-\\nlength input sequences into fixed-length output query representa-\\ntions. It was initially proposed for visual-text modality alignment,\\nand here is applied to audio-text alignment. In each Q-Former block,\\ntrainable query embeddings Q∈Rnq×dqinteract with the input fea-', metadata={'doc_id': 'ee1bd7b1-3660-47fa-ae10-ad681c76a289', 'page': 1, 'source': 'pdfs/2309.13963.pdf'})]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch.similarity_search(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='3. MODULE CONNECTOR\\nAs shown in Fig. 1, the proposed ASR model consists of three mod-\\nules: a frozen speech encoder, a trainable module connector and\\na frozen LLM. This section introduces three connectors, including\\nfully connected layers, multi-head cross-attention and Q-Former.\\n❄ Speech EncoderLinearMHCA\\n❄ Large Language Model(a)(b)(c)Transcription\\nQ-FormerQ-Former queryVicuna embeddingConv kernel\\nLinear\\nFig. 1 . Illustration of integrating a speech encoder and an LLM into\\nan ASR system with a module connector of: (a) fully connected\\nlayers, (b) multi-head cross-attention, and (c) Q-Former.\\nFor clarity, some basic notations are defined as follows: X∈\\nRnx×dxdenotes the speech features obtained from the speech en-\\ncoder, and the module connector compresses XintoTspeech∈\\nRnt×dtwhich are input to the LLM to produce ASR transcriptions.\\nH∈Rnh×dhdenotes the hidden states in connectors while nandd\\nare the numbers of vectors and hidden dimensions respectively.\\n3.1. Fully connected layers\\nTo compress the length of speech features, madjacent frames xi,\\nxi+1, ...,xi+m−1are stacked into hi∈Rm×dx. Then two Linear (·)\\nlayers with ReLU (·)in between are introduced as follows:\\nTspeech=Linear (ReLU (Linear (H))), (1)\\nwhere Hconsists of hiof a batch of samples. Actually, the vector\\nstacking operation together with the first linear layer works the same\\nas a 1-dimensional (-d) convolutional layer, Conv1d (·).\\n3.2. Multi-head cross-attention\\nTo bridge the gap between the multi-modal encoder output features\\nXand LLM input textual features Tspeech, a multi-head attention\\nlayer [33] denoted as MultiHead (Query ,Key,Value )is used in the\\nmulti-head cross-attention approach to align the two feature spaces\\n[19]. First, a Conv1d (·)layer reduces the length of the speech input\\nby a rate of s. Then the hidden states Hare converted to Tspeech\\nbased on the textual embeddings Eusing MultiHead (·). That is,\\nH=Linear (Conv1d (X)) (2)\\nTspeech=MultiHead (H,E,E). (3)3.3. Q-Former\\nQ-Former [20] is a Transformer-based module converting variable-\\nlength input sequences into fixed-length output query representa-\\ntions. It was initially proposed for visual-text modality alignment,\\nand here is applied to audio-text alignment. In each Q-Former block,\\ntrainable query embeddings Q∈Rnq×dqinteract with the input fea-\\nturesXthrough multi-head self-attention and cross-attention layers,\\nSpecifically, Q-Former, denoted as QF (Q,X), in this work consists\\nof two Transformer decoder blocks [33] with the causal attention\\nmasks removed. Here Qis used as the decoder inputs and Xas the\\nencoder outputs in the standard Transformer.\\n4. SEGMENT-LEVEL Q-FORMER\\nTransformer-based speech encoders can have limitations on the in-\\nput sequence duration [18]. To enable LLMs to process with longer\\nspeech inputs, the whole sequence can be split into several shorter\\nsegments to transform by the speech encoder separately. Such seg-\\nments can be concatenated to reform a single sequence at either\\nthe input or output end of Q-Former. In this paper, the structure\\nof segment-level Q-Former (seg-QF) shown in Fig. 2 is proposed,\\nwhich uses a Q-Former to transform each encoder output segment\\nsimultaneously and concatenates their fixed-length output token se-\\nquences before feeding into the LLM. Compared to performing the\\nconcatenation at the Q-Former input end and producing a fixed num-\\nber of nqoutput tokens, seg-QF allows varying the number of out-\\nput tokens N×nqaccording to the number of segments N, which\\nis more suitable for speech inputs with variable lengths in a wide\\nrange. Note the trainable query embeddings Qand Q-Former lay-\\ners are shared among all the segments, and seg-QF can be initialised\\nwith a pre-trained standard Q-Former.\\n❄ Speech Encoder12Seg-QF\\n❄ Large Language ModelTranscription\\n………pos:pos:\\nFig. 2 . The model structure of segment-level Q-Former (seg-QF).\\nThe integers in rectangles are segment-level positional encodings.\\nDespite that relative positions of the frames are provided by the\\nspeech encoder within each segment Si∈Rnx×dx, Seg-QF is not\\naware of their absolute positions in the whole input sequence. To\\ninform Seg-QF with such information, segment-level position em-\\nbeddings pi∈Rdxare added to X, as shown in Fig. 2. Specifically,\\nTspeech= [QF(Q,Si⊕pi)]N\\ni=1, (4)\\nwhere Si⊕qimeans adding qito each row of Si.', metadata={'source': 'pdfs/2309.13963.pdf', 'page': 1}),\n",
       " Document(page_content='CONNECTING SPEECH ENCODER AND LARGE LANGUAGE MODEL FOR ASR\\nWenyi Yu1, Changli Tang1, Guangzhi Sun1, Xianzhao Chen2,\\nTian Tan2, Wei Li2, Lu Lu2, Zejun Ma2, Chao Zhang1,∗\\n1Department of Electronic Engineering, Tsinghua University,2ByteDance\\nywy22@mails.tsinghua.edu.cn; cz277@tsinghua.edu.cn\\nABSTRACT\\nThe impressive capability and versatility of large language mod-\\nels (LLMs) have aroused increasing attention in automatic speech\\nrecognition (ASR), with several pioneering studies attempting to\\nbuild integrated ASR models by connecting a speech encoder with\\nan LLM. This paper presents a comparative study of three com-\\nmonly used structures as connectors, including fully connected\\nlayers, multi-head cross-attention, and Q-Former. Speech encoders\\nfrom the Whisper model series as well as LLMs from the Vicuna\\nmodel series with different model sizes were studied. Experiments\\nwere performed on the commonly used LibriSpeech, Common\\nV oice, and GigaSpeech datasets, where the LLMs with Q-Formers\\ndemonstrated consistent and considerable word error rate (WER)\\nreductions over LLMs with other connector structures. Q-Former-\\nbased LLMs can generalise well to out-of-domain datasets, where\\n12% relative WER reductions over the Whisper baseline ASR model\\nwere achieved on the Eval2000 test set without using any in-domain\\ntraining data from Switchboard. Moreover, a novel segment-level\\nQ-Former is proposed to enable LLMs to recognise speech seg-\\nments with a duration exceeding the limitation of the encoders,\\nwhich results in 17% relative WER reductions over other connector\\nstructures on 90-second-long speech data.\\nIndex Terms —Large language model, automatic speech recog-\\nnition, Q-Former, long-form speech\\n1. INTRODUCTION\\nLarge language models (LLMs) [1–6] with rich knowledge and the\\nability to solve novel and complex tasks have revolutionised the field\\nof natural language processing. More recently, significant attention\\nhas been drawn to enable LLMs to handle speech inputs [7–17]. In\\naddition to pipeline-based methods in which the LLM serve as a con-\\ntroller to manage a set of functional models [7, 8], two categories of\\napproaches have been developed. The first category of approaches\\ndiscretises speech inputs and embeds the derived speech tokens into\\na vector space shared with the text tokens, then the LLMs are fine-\\ntuned to fit into this new token space [9, 10]. The other category of\\napproaches directly connects a speech encoder with an LLM using\\na connector that aligns the speech encoders with the LLMs [11–15].\\nThis paper focuses on the second category of approaches.\\nWhen aligning the speech encoder output and LLM input spaces,\\nthe choice of the connector is of vital importance, and should meet\\nthe following requirements. First, the connector should be able to\\nretain as much information from the speech inputs as possible, since\\nit determines the amount of information that LLMs can receive from\\nthe speech. Second, as the computation and storage costs of LLMs\\n∗Corresponding authorincrease considerably when processing long input sequences, the\\nconnector should be able to achieve efficient and effective informa-\\ntion compression to reduce the lengths of the LLM input sequences.\\nFocusing on automatic speech recognition (ASR) to show the\\nability of LLMs to recognise speech, this paper studies different\\nstructures of connectors that integrate LLMs with speech encoders.\\nSpecifically, an end-to-end ASR system was constructed by con-\\nnecting a Whisper model encoder [18] with a Vicuna LLM [5].\\nThree types of connectors were compared in this paper, including\\nthe fully connected layers, multi-head cross-attention [19] and Q-\\nFormer [20]. To bypass the input length limitation of the pre-trained\\nspeech encoders and enable the LLM to process long speech inputs, a\\nnovel segment-level Q-Former connector is proposed. Experiments\\nwere conducted based on a training set with ∼4,000 hours data, and\\nthe LLMs with Q-Former connectors consistently outperform strong\\nWhisper baseline ASR systems on the in-domain datasets, and can\\nachieve over 12% relative word error rate (WER) reductions on\\nthe out-of-domain Eval2000 test set from the Switchboard corpus.\\nThe influence of the number of connector output tokens and model\\nsize are studied. Moreover, the proposed segment-level Q-Former\\nstructure achieved obvious WER reductions on long speech inputs,\\nwhen compared with other connectors.\\nThe rest paper is organised as follows. Sec. 2 summarises the\\nwork related to multimodal LLMs. Secs. 3 and 4 introduce the three\\nconnectors to compare and the proposed segment-level Q-Former.\\nThe experimental setup and results are presented in Secs. 5 and 6,\\nfollowed by conclusions.\\n2. RELATED WORK\\nTo enable LLMs to perform both speech perception and generation,\\nSpeechGPT [9] and AudioPaLM [10] augment the vocabularies of\\nLLaMA [4] and PaLM [3] LLMs with discrete speech tokens ex-\\ntracted by HuBERT [21] and W2v-BERT [22] or USM [23] speech\\nencoders respectively. Regarding the approaches to connect multi-\\nmodal encoders to LLMs, X-LLM [11] interfaced ChatGLM with\\naudio and visual encoders. [14] and [13] connect speech encoders\\nwith reduced frame rates to LLMs, and achieve integrated multilin-\\ngual ASR and speech translation respectively. Moreover, LLMs can\\nalso be prompt for domain adaptation [15] and uncertainty estima-\\ntion [17] of the ASR results.\\nSeveral works studied visual LLMs [19, 20, 24–30]. Following\\nBLIP-2 [20], InstructBLIP [26] and Video-LLaMA [27] introduced\\nQ-Former as the module connector. Alternative connectors were also\\ninvestigated in [19,24,28,29]. Regarding audio and music LLMs, the\\nreasoning ability based on audio is studied [31]. MU-LLaMA [32]\\nshowed outstanding music understanding abilities.arXiv:2309.13963v2  [eess.AS]  26 Sep 2023', metadata={'source': 'pdfs/2309.13963.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = \"\"\n",
    "for res in chain.stream({\"context\": contexts_formatter(contexts), \"question\": question, \"chat_history\": memory.buffer_as_messages}):\n",
    "    if res:\n",
    "        print(res.content, end=\"\", flush=True)\n",
    "        answer += res.content\n",
    "\n",
    "# with get_openai_callback() as cb:\n",
    "#     res_invoke = chain.invoke({\"context\": contexts_formatter(contexts), \"question\": question, \"chat_history\": memory.buffer_as_messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Berdasarkan konteks di atas, Q-Former adalah sebuah modul yang berbasis Transformer yang digunakan untuk mengubah urutan masukan yang berkepanjangan menjadi representasi kueri (query) dengan panjang tetap. Modul ini awalnya dikembangkan untuk pencocokan modalitas visual-teks, tetapi dalam konteks ini, Q-Former digunakan untuk mencocokkan audio-teks. Dalam blok Q-Former, embedding kueri yang dapat dilatih (trainable query embeddings) berinteraksi dengan fitur masukan melalui lapisan self-attention dan cross-attention multi-head. Q-Former ini terdiri dari dua blok decoder Transformer dengan masker perhatian kausal dihilangkan.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template prompt\n",
    "template_reference = \"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Based on the question and answer pair above, find where the answer originates from within the given context.\n",
    "\n",
    "Return a JSON object with the following keys:\n",
    "    `page`: (the value is in the form of a list of page numbers, can be more than one page)\n",
    "    `source`: (the value is in the form of a list of sentences used as a reference for the answer, write exactly as it appears in the context including the in-text citation, using the language used in the context)\n",
    "    `in-text citation` : (list of in-text citation appears in contexts used as reference for the answer)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt_reference = PromptTemplate.from_template(template_reference)\n",
    "\n",
    "# Output parser\n",
    "json_parser = SimpleJsonOutputParser()\n",
    "\n",
    "# LCEL\n",
    "reference_chain = prompt_reference | llm | json_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = reference_chain.invoke({\n",
    "    \"context\": contexts_formatter(contexts), \n",
    "    \"question\": question, \n",
    "    \"answer\": answer\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import pandas as pd\n",
    "\n",
    "matches = list()\n",
    "pages = list()\n",
    "for i in range(3):\n",
    "    m = [m for m in SequenceMatcher(None, contexts[i].page_content, reference[\"source\"][0]).get_matching_blocks() if m.size > 15]\n",
    "    if m:\n",
    "        # m = contexts[i].page_content[m[0].a : m[-1].a + m[-1].size]\n",
    "        # matches.append(m)\n",
    "        for j in range(len(m)):\n",
    "            matches.append(contexts[i].page_content[m[j].a : m[j].a + m[j].size])\n",
    "            pages.append(contexts[i].metadata[\"page\"])\n",
    "\n",
    "df_sources = pd.DataFrame({\"match\": matches, \"page\": pages})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def dict_to_string(input_dict):\n",
    "    json_string = json.dumps(input_dict, indent=2)  # indent for pretty formatting (optional)\n",
    "    return json_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex_python.converter import CurrencyRates\n",
    "\n",
    "def get_cost(\n",
    "        model=\"gpt-3.5-turbo\", \n",
    "        prompt_formatted=prompt.format(context=contexts_formatter(contexts), question=question, chat_history=memory.buffer_as_messages),\n",
    "        output_from_llm=dict_to_string(res)\n",
    "):\n",
    "    curr = CurrencyRates()\n",
    "    encoder = tiktoken.encoding_for_model(model)\n",
    "    input_tokens_used = len(encoder.encode(prompt_formatted)) + 7 # Jaga-jaga\n",
    "    output_tokens_used = len(encoder.encode(output_from_llm))\n",
    "    total_token = input_tokens_used + output_tokens_used\n",
    "\n",
    "    input_price = round((0.0015/1000) * input_tokens_used, 8)\n",
    "    output_price = round((0.002/1000) * output_tokens_used, 8)\n",
    "    total_price_usd = round(input_price + output_price, 8)\n",
    "    total_price_idr = curr.convert('USD', 'IDR', total_price_usd)\n",
    "\n",
    "\n",
    "    return f\"\"\"Tokens Used: {total_token}\n",
    "        Prompt Tokens: {input_tokens_used}\n",
    "        Completion Tokens: {output_tokens_used}\n",
    "    Total Cost (USD): ${total_price_usd}\n",
    "    Total Cost (IDR): Rp{total_price_idr}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlighting Sources\n",
    "def get_matches(source, contexts=contexts, k=3):\n",
    "    for i in range(k):\n",
    "        idx_awal = contexts[i].page_content.find(source)\n",
    "        if idx_awal != -1:\n",
    "            idx_akhir = contexts[i].page_content[idx_awal:].find(\".\")\n",
    "            idx_akhir += idx_awal\n",
    "        \n",
    "            match_ = contexts[i].page_content[idx_awal:idx_akhir]\n",
    "            if len(match_) > 10:\n",
    "                page_num = contexts[i].metadata[\"page\"]\n",
    "                return contexts[i].page_content[idx_awal:idx_akhir], page_num\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def highlight_pdf(path, match_, page_num, output_path):\n",
    "    pdf = fitz.open(path)\n",
    "    page = pdf[page_num]\n",
    "\n",
    "    matches = page.search_for(match_.replace(\"-\\n\", \"\"))\n",
    "\n",
    "    for m in matches:\n",
    "        page.add_highlight_annot(m)\n",
    "\n",
    "    pdf.save(output_path)\n",
    "    pdf.close()\n",
    "\n",
    "def highlight_pdf_v2(path, df_sources, output_path):\n",
    "    for i in range(len(df_sources)):\n",
    "        if i == 0:\n",
    "            pdf = fitz.open(path)\n",
    "            page = pdf[df_sources[\"page\"].loc[i]]\n",
    "\n",
    "            matches = page.search_for(df_sources[\"match\"].loc[i].replace(\"-\\n\", \"\"))\n",
    "\n",
    "            for m in matches:\n",
    "                page.add_highlight_annot(m)\n",
    "\n",
    "        else:\n",
    "            pdf = fitz.open(path)\n",
    "            page = pdf[df_sources[\"page\"].loc[i]]\n",
    "\n",
    "            matches = page.search_for(df_sources[\"match\"].loc[i].replace(\"-\\n\", \"\"))\n",
    "\n",
    "            for m in matches:\n",
    "                page.add_highlight_annot(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get references\n",
    "def get_reference(docs, in_text_citation):\n",
    "    model = OpenAI()\n",
    "\n",
    "    get_citation_template = \"\"\"From the reference list below, rewrite the specified references!\n",
    "    References:\n",
    "    {references}\n",
    "\n",
    "    Please rewrite the references related to the following numbers:\n",
    "    {in_text_citation}\n",
    "    \"\"\"\n",
    "\n",
    "    GET_CITATION_PROMPT = PromptTemplate.from_template(get_citation_template)\n",
    "\n",
    "    get_reference_chain = chain = GET_CITATION_PROMPT | model\n",
    "\n",
    "    for page_number in range(len(docs)):\n",
    "        page = docs[page_number]\n",
    "        text = page.page_content\n",
    "\n",
    "        if \"References\" in text or \"REFERENCES\" in text:\n",
    "            references_text = text.split(\"References\")[1] if \"References\" in text else text.split(\"REFERENCES\")[1]\n",
    "\n",
    "    result = get_reference_chain.invoke({\"references\": references_text, \"in_text_citation\":in_text_citation})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if res[\"source\"]:\n",
    "    match_, page_num = get_matches(res[\"source\"][:15], contexts)\n",
    "\n",
    "    if match_:\n",
    "        highlight_pdf(\"pdfs/2309.13963.pdf\", match_, page_num, \"pdfs/highlighted/high_pdf.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20] J. Li, D. Li, S. Savarese, and S. Hoi, “BLIP-2: Bootstrapping Language-Image Pre-Training with Frozen Image Encoders and Large Language Models,” in Proc. ICML , Vienna, 2023.\n"
     ]
    }
   ],
   "source": [
    "if reference[\"in-text citation\"]:\n",
    "    ref_result = get_reference(docs, reference[\"in-text citation\"]).strip()\n",
    "print(ref_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
